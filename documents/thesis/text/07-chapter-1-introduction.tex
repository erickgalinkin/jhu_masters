\chapter{Introduction}
\label{chap:intro}


\sect{Neural Networks}
Neural Networks are a type of connectionist machine learning system in which artificial neurons are connected to one another in an attempt to emulate animal - particularly human - brains.
Deep learning is a particular form of machine learning in which artificial neurons are stacked several layers deep to learn more complex representations. 
The recent boom in deep learning stems from the truly groundbreaking work advancing the state-of-the-art in natural language processing and computer vision domains.
This has reasonably led us to ask what the applicability of neural networks and deep learning are to other domains. 

\sect{Previous Work}
The core result relies on prior work by Kraskov \textit{et al.}~\cite{kraskov2004estimating}. 
In this work, Kraskov \textit{et al.} present methods for estimating mutual information, which we use to empirically validate our results about preservation of mutual information.
A special case of their proof of invariance of mutual information under homeomorphism is proven in \ref{chap:four}, and is used to support our key results.
Our work differs significantly in that we prove empirical results which support their theory, and we contextualize the results by showing how they relate to neural networks.

Our work also leverages an expanded dataset from Watkins \textit{et al.}~\cite{watkins2013using} and one of our objectives, as in Watkins' work, was to build a model for mobile malware detection.
In the Watkins literature, a decision tree algorithm from the WEKA package was used to classify traffic. 
In order to compare to Watkins' results to our own neural network results, we leverage a random forest from the Scikit-learn~\cite{scikit-learn} Python package to serve as a baseline.

Deep neural networks have been applied to related problems in information security, particularly detecting malicious executables on endpoints~\cite{raff2018malware} using techniques from natural language processing and computer vision.
This research worked well at the endpoint, examining static properties of malicious and benign executables to draw determinations about the software from byte sequences.

The use of Fourier transforms in neural networks has been of interest for some time and there are several papers on the subject~\cite{osowski2002fourier, pratt2017fcnn, highlander2016very} which consider these applications.
The choice to explore Fourier transforms in convolutional neural networks is natural, as the dot product is much faster than a convolution operation which relies on a sliding kernel. 
We lean most heavily on the paper by Pratt \textit{et al.}~\cite{pratt2017fcnn} due to its recency and implementation details. 
Particularly, Pratt considers the impact of the convolution theorem within neural networks and uses the Fast Fourier Transform to quickly compute $\mathcal{F}(\kappa * u) = \mathcal{F}(\kappa) \odot \mathcal{F}(u)$ where $\mathcal{F}$ is the Fast Fourier Transform, $*$ denotes convolution, and $\odot$ denotes the Hadamard Pointwise Product.
They also describe a Fourier Pooling Layer to reduce the data size while retaining information by truncating the boundaries of the matrices.
Ultimately, the paper shows that on the CIFAR-10 and MNIST datasets, the overall accuracy is lower than benchmark results - though the network trains and evaluates images much more quickly.
Interestingly, we found the opposite results, which we detail in chapter~\ref{chap:three}.

Wavelet neural networks pioneered by Fujieda \textit{et al.}~\cite{fujieda2017wavelet} have shown promise for generalized pooling and convolution by abstracting them into downsampling and filtering in the spectral domain.
The results in the Fujieda paper were significant, as the network achieved better accuracy results than AlexNet on the target dataset while having approximately 1/4 the number of parameters.
In addition, the memory requirements and speed of the network were a significant improvement on the other architectures considered by Fujieda.
It is worth noting that implementation details from Fujieda are sparse, and so our implementation may differ from this reference implementation in some way, though the spirit and overall methods are the same.

