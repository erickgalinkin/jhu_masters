\chapter{Preliminaries}
\label{chap:two}
%\newpage

\section{A Whirlwind Tour of Information Theory} 
Since our work makes use of information theory, it is helpful to cover the core terminology.
For the following proofs, all random variables are assumed to be discrete. 
This is both because binary computers have only finite precision, and are thus not ``truly'' continuous but also because discrete information theory is a far more mature science in that many foundational results are proven only in the discrete case.

First, we define entropy as the measure of uncertainty of a single random variable.
Let $X$ be a random variable with alphabet $\mathcal{X}$ and probability mass function $p(x) = \\Pr\{X = x\}, x \in \mathcal{X}$.
Then the entropy of $X$, represented $H(X)$ is defined as
$$H(X) = -\sum_{x \in \mathcal{X}} p(x) \log{p(x)}$$
Here our logarithm is to the base 2, as information is most commonly represented as bits.
We maintain this definition of the logarithm throughout. 

In the case where we are examining two random variables, for example, a dataset and its labels, we may want to consider the joint and conditional entropy of those random variables.
The joint density of a pair of discrete random variables $(X, Y)$ with joint distribution $p(X, Y)$ is:
$$H(X, Y) = - \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log{p(x, y)}$$

and the differential entropy $H(Y | X)$ as:
$$ h(Y|X) = -\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log{p(y|x)}$$

We define the Kullback-Leibler divergence $D(f||g)$ between two probability mass functions $p(x)$ and $q(x)$ to be:
$$D(f||g) = \sum_{x \in \mathcal{X}} p(x) \log{\frac{p(x)}{q(x)}}$$

Therefore, we define mutual information, $I(X; Y)$ as the relative entropy between the joint distribution and the product distribution.
$$I(X; Y) = \sum_{x, y} p(x, y) \log{\frac{p(x,y)}{p(x) p(y)}} = H(Y) - H(Y|X)$$

Detailed derivation of the above results are available in \cite{coverthomas2006}.

\section{Invariance of Mutual Information in Neural Networks}\label{invariance}

We begin by proving the invariance of mutual information under homeomorphism.

\begin{theorem}[Invariance of Mutual Information under Homeomorphism]
Take two random variables $X$ and $Y$ where $Y$ is the set of labels of $X$.
Let $X' = \psi(X)$, where $\psi$ is a smooth and uniquely invertible map.
Then $I(X'; Y) = I(X; Y)$.
\end{theorem}

\begin{proof}
Given the Jacobi determinant $J_X = ||\partial X/ \partial X'||$, we observe that the joint distribution of $X'$ and $Y$ is given by: $f_{X', Y}(x', y) = J_X(x')f(x, y)$

\begin{align*}
I(X'; Y) & = \int \int f(x', y) \log \frac{f(x', y)}{f_{X'}(x') f_{Y}(y)} dy dx'\\
& = \int \int f(x, y) \log \frac{f(x, y)}{f_{X}(x) f_{Y}(y)} dy dx \\
& = I(X; Y)
\end{align*}
\end{proof}

\section{Minimum Necessary Information and Information Bottleneck}
Naftali Tishby and Noga Zaslavsky introduced the information bottleneck theory of neural networks~\cite{tishby2015deep} as a way of explaining the theoretical generalization bounds of neural networks.
In particular, Tishby and Zaslavsky show that any deep neural network can be quantified by the mutual information between the input, hidden layers, and the output variable by way of information per the data processing inequality.
Neural networks satisfy the information bottleneck optimality equation
$$\min_{p(z|x):Y \to X \to Z} I(Z;X) - \beta I(Z; Y) , \beta > 0$$
The information bottleneck learns the representation $Z$ subject to the above constraint, where $\beta$ controls the strength of the constraint.
The standard cross-entropy loss is recovered as $\beta \to \infty$. 
Further work by Alemi \textit{et al.}~\cite{alemi2016deep} suggests further refinements on the information bottleneck theory which we do not discuss in detail here. 

The Minimum Necessary Information as defined by Fischer~\cite{fischer2020conditional} consists of three components for a learned representation:
\begin{enumerate}
	\item \textbf{Information} We would like a representation $Z$ which captures useful information about a dataset $(X, Y)$.
	\item \textbf{Necessity} The value of information to accomplish a task. In this case, predicting $Y$ given $X$ using our representation $Z$. That is, $I(X; Y \leq I(Y; Z)$
	\item \textbf{Minimality} Given all representations that can solve the task, we prefer the one which retains the smallest amount of mutual information. That is, $I(X; Y) \geq I(X; Z)$.
\end{enumerate}
Using Fischer's definitions of necessity and minimality, we see that there is a point which he calls the ``MNI Point'':
$$I(X; Y) = I(X; Z) = I(Y; Z)$$
This equation may not be satisfiable, since for any representation $Z$ given a dataset $(X, Y)$, there is a maximum value we are subject to:
$$1 \geq D(X||Z) = \sup_{Z \leftarrow X \rightarrow Y}\frac{I(Y; Z)}{I(X; Z)}$$
We achieve equality if and only if $X \to Y$ is a deterministic map.

\section{Information Geometry of Neural Networks}
A neural network, as mentioned briefly in chapter \ref{intro:nn}, is a form of connectionist machine learning which is a universal approximator under minor assumptions about the activation function~\cite{goodfellow2016deep}.
In particular, a neural network connects many artificial neurons which receive input $x$ and emit output a prediction of $y$. 
The forward pass of a single neuron gives us:
$$y = \sigma(w \cdot x + \beta)$$
Where $\sigma$ is an activation function meeting the aforementioned assumptions, and $\beta$ is a bias vector which can be set to zero and not learned.
For simplicity's sake, $\beta$ is omitted and assumed to be zero from this point without loss of generality.
Then a two-layer neural network can be completely specified by layer weights:
$$\xi = (w_1, ..., w_m; v_1, ..., v_m)$$
and our simplified neural network output is given by:
$$y = f(x, \xi) = v\sigma(w \cdot x)$$

Letting $M$ be the parameter space of all neural networks allows us to specify $\xi$ as a coordinate system over $M$, which is a manifold of dimension $N=(n+1)m$, where $n$ is the dimension of the input, and $m$ is the number of neurons in the hidden layer.
This manifold is where learning occurs, with $\xi$ . 
% Since this manifold contains sets of points $\xi \neq \xi'$ for which $f(x, \xi) = f(x, \xi')$, then we cannot uniquely identify $\xi$ from the output. 
% This means we have some $\xi$ for which $f(x, \xi) = 0$ and so either $v$ or $w$ is the zero vector.
% Given this definition, we specify the critical region $R$ of $M$:
% $$R = \{\xi | v = 0 \text{ or } w = 0\}$$

Given the above results, it seems prudent to consider that by definition of homeomorphism, any smooth, bijective map applied to our dataset will only deform the manifold which is parameterized by $\xi$. 
Since we optimize $\xi$ with respect to $I(X; Z)$, it stands to reason that as we approach the MNI point and our loss function approaches zero and our estimator $\xi$ approaches the Cram\'{e}r-Rao bound.`
Additionally, we conclude that since both $M$ and mutual information are invariant to homeomorphism, then the results of training via stochastic gradient descent are also unaffected by homeomorphic transformation.
This theory is borne out and discussed in the context of our experiments in chapter \ref{chap:conclusion}