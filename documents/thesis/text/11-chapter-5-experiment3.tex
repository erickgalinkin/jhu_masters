\chapter{Experiment 3 - In-Network Malware Dataset Transformation}
\label{chap:four}

In this experiment, we consider the work of Pratt~\cite{pratt2017fcnn} and Fujieda~\cite{fujieda2017wavelet} under the mutual information theory.
Both the Fourier and Wavelet neural networks take advantage of the convolution theorem - that is, given two functions $f$ and $g$,
\begin{align}
(f * g)(t) & = \int_{-\infty}^{\infty} f(\tau)g(t-\tau)dt \\
& = \int_{\mathbb{R}^n}f(x) e^{-2\pi i x \nu} dx	 \cdot \int_{\mathbb{R}^n}g(x) e^{-2\pi i x \nu} dx\\	
& = \mathcal{F}\{f\}(\nu) \cdot \mathcal{F}\{g\}(\nu)
\end{align}

and in the inverse, we get:
$$f \cdot g = \mathcal{F}^{-1}\{\mathcal{F}\{f\} * \mathcal{F}\{g\}\}$$

This allows us to avoid the high computational cost of performing a convolution via the sliding-tile method and instead potentially take advantage of the convolution theorem to perform convolution at the speed of the dot product.

\section{Methodology}
In this experiment, we use the same raw dataset, fully-connected neural network, and convolutional neural network as before, and we report the results of the ``control group'', the support vector classifier and random forest.
The results of these experiments are verbatim from \ref{chap:three}.
We also introduce two additional models to evaluate the effect of transformation on the learned representations.
The Fourier network and wavelet network differ from a conventional convolutional neural network by performing an in-layer transformation before the activation function is applied. 

\section{Results}
\begin{table}[ht]
\caption{Neural Network Results}
\centering
\label{Tab:test}	
\begin{tabular}{l|ll}
\textbf{Architecture}  & \textbf{Test Accuracy} & \textbf{Mean Step Time} ($\mu$s) \\\cline{1-3}
Fully-Connected NN            & 63.40\%         & 13\\
Convolutional NN              & 72.89\%         & 54\\  
Fourier NN                    & 63.27\%         & 143\\
Wavelet NN                    & 74.85\%         & 228\\
Random Forest                 & 80.28\%         & N/A\\ 
Support Vector Classifier     & 65.77\%         & N/A       
\end{tabular}
\end{table}

Again, the random forest outperforms all of our other classifiers.
It is worth noting that one of the primary motivations for replacing the sliding-tile convolution method with a Fourier or Wavelet method is the performance.
However, as we observe, the Fourier and Wavelet networks are significantly slower than their untransformed counterparts.
We conclude that the computational overhead of performing a transform and its corresponding inverse transform outweighs the speed-up gained by eliminating the sliding-tile convolution on smaller datasets, and the method as demonstrated in Pratt~\cite{pratt2017fcnn} should be reserved for relatively large images, where convolution is extremely slow.
In our case, we see a 2.65x increase in step time between a standard convolution and the Fourier method. 
Unfortunately, our activation functions do not behave nicely in the Fourier or Wavelet domain, as they operate linearly with respect to the space. 
The question of using a novel convolution operator and conducting the activation in that space has been addressed by Chakraborty~\cite{chakraborty2019surreal} but goes well beyond the question of simply adapting an activation function to the Fourier or Wavelet space.
The search for a "spectral activation" function is still an open question.