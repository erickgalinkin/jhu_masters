\chapter{Experiment 1 - Watkins Malware Dataset}
\label{chap:three}
% Introduction
Neural networks have demonstrated some success in the security domain~\cite{raff2018malware} and so we have applied them to the Watkins~\cite{watkins2013using} dataset.
This dataset consists of interarrival times for packets sent to Android devices, some of which were running malware.
Detecting malware via network traffic is an important problem, and this interpretation of the problem is crucial for addressing the situation where a network owner cannot install an antivirus agent on a device which may be infected with malware.
This is a common situation when personal devices are introduced to a corporate network; being able to detect malicious software on a device without having an agent on the device provides a tremendous benefit to network defenders.

\section{Methodology}
This experiment proceeds in three parts.
The first considers our standard models: fully-connected neural network, convolutional neural network, support vector machine, and random forest.
These models process our malware datasets in alignment with the methods elaborated in Section~\ref{data}.
In experiment 1.a, the neural networks are trained for a maximum of 30 epochs and we use the early stopping technique to prevent overfitting.
Early stopping will end training early when some condition is met - in our case, we stop early if the network's loss has not decreased by 0.001 or more for two consecutive training epochs.

In experiment 1.b, we consider only the raw data across the standard models as well as the Fourier neural network and wavelet neural network.
The Fourier neural network and wavelet neural network differ from a conventional convolutional neural network by performing an in-layer transformation before the activation function is applied, exploiting the convolution theorem.
As in our previous experiment, order to ensure that deviations in the dataset did not induce significant variation in accuracy, 100 trials were run, and the accuracy and mean step time for all trials was averaged.

In experiment 1.c, the network is trained for 1000 epochs without early stopping, and at each training epoch, the mutual information between the labels and the network, $I(Y; M)$ and the mutual information between the data and the network, $I(X; M)$ is computed.
This differs from the previous experiment as we do not concern ourselves with accuracy but instead wish to see and plot the change in mutual information during training.
By running for 1000 epochs, we significantly overfit the training set, making this a poor approach for optimizing accuracy.
We consider only the first and last layers of the neural network since the information tends in the same direction, with the propagation of information going from the output layer back through the intermediate layers.
By plotting only the first and last layers in Figures~\ref{fig:malware infoplane fc} and \ref{fig:malware infoplane conv}, we end up with a more interpretable plot.
The details of this plot are described below in~\ref{infoplane}

\subsection{Mutual Information Computation}\label{MI computation}
In each experiment, the following data are collected for each epoch:
\begin{enumerate}
	\item The L2 norm of the weights
	\item The mean of the gradients
	\item The standard deviation of the gradients
	\item The post-activation output of each layer for the test set. 
\end{enumerate}
These data are then stored in a file.
After training, the data is loaded from the files.
The entropy of the activity is computed by considering the KL-based upper bound on the entropy using techniques from section 4 of Kolchinsky and Tracey~\cite{kolchinsky2017estimating} to yield the entropy of the layer $H(M)$.
This estimate is:
\begin{equation}
	H(M) = -\sum_{i} p_i \ln \sum_{j} p_j \exp(-D(m_i || m_j))
\end{equation}
where $p$ is either the probability density of the dataset, $X$, or the probability of the label $Y$ and $m$ is the probability density estimate of our network layer, $M$.
This number is recorded and stored.
Using the activity output recorded during training and the entropy of the data, the conditional entropy given the data, the conditional entropy $H(M|X)$ is computed by techniques in section 5 of Kolchinsky and Tracey
Then, we compute $I(X;M) = H(M) - H(M|X)$ to provide our mutual information.

For the entropy with respect to the labels, individual label probabilities are computed and used with the entropy of the activity to compute the conditional entropy of the activity given the label probabilities, giving us $H(M|Y)$
This is used in conjunction with our earlier computation of $H(M)$ so that we can compute the mutual information $I(Y; M) = H(M) - H(M|Y)$.
 
These two mutual information values are then used to display information plane information as plotted in Figure~\ref{fig:malware infoplane fc}, Figure~\ref{fig:mnist fc infoplane} and others.
This is identical to the method used in Saxe~\cite{saxe2019information}.

\section{Data}\label{data}
We leveraged four different datasets: Raw, Fourier-transformed, Wavelet-transformed, and a dataset consisting of summary statistics.
The summary statistics of the first three are captured in Table~\ref{Tab:summary} - we did not compute summary statistics for the dataset of summary statistics.  
Our data consisted of 98 legitimate applications and 120 pieces of malware, which were collected by Yu and Li~\cite{yu2018network}.
This gives us a dataset which is approximately 55\% malware and 45\% benignware.
While this distribution is not reflective of real environments where malware is significantly rarer than benign applications, we do not adjust for this disparity since our tolerance for alerting on benign applications is much higher than our tolerance for not detecting malicious applications. 
For each application, five trials were conducted where the interarrival time was collected for each of 100 ICMP ping packets, yielding a total dataset of 1090 trials.
Further details of the data collection can be found in~\cite{yu2018network, watkins2013using}.
One interesting effect of performing the transforms on the dataset is that while the continuous wavelet transform reduces our variance significantly and slightly normalizes the dataset, the Fourier transform has the opposite effect, introducing tremendous amounts of noise into the dataset.

\subsection{Raw data}
This is the data as described above, captured by Yu and Li in accordance with Watkins~\cite{watkins2013using}.
In this dataset, only the raw measurements are used in a 100-dimensional row vector, with a label of 0 for benign and 1 for malicious.

\subsection{Fourier data}
The Fourier data is a copy of the raw data under the Fourier transform.
In particular, since our raw data is given by a single 100-dimensional row vector, it is a direct mapping of that row vector under the Fast Fourier Transform as provided by the numpy library.

\subsection{Wavelet data}
The wavelet dataset is a copy of the raw data under a continuous wavelet transform. 
The Morlet wavelet is used for the transform for several reasons:
First, it is a wavelet which allows us to maintain the dimensionality of our data, making it easier to compare in performance and to re-use neural network architectures.
Secondly, the Morlet wavelet is closely related to human perception~\cite{mallat1999wavelet, daugman1985uncertainty}, providing a small connection to the human brain conception of neural networks.
Lastly, the Morlet wavelet is uniquely invertible, which is not the case for all potential mother wavelets. 
%Since both detail and approximation coefficients are produced by the wavelet transform, but the detail coefficient-only results were identical to the results where both were used in all trials, the two-channel data was used.
%This likely relates to the volume of high-frequency parameters in the parameter space~\cite{rahaman2018spectral} and the vanishing gradients experienced by the low-pass approximation coefficient values close to zero.
%Further investigation of the density of information in different bands of the signal has been conducted by de Oliveira \textit{et al.}~\cite{de2006wavelet} but is outside the scope of this work.

\subsection{Summary data}
The summary data leverages the seven features used by Yu and Li: arithmetic mean, standard deviation, variance, maximum, minimum, geometric mean, and harmonic mean. 
These features were fed to the classifier based on the sample they were captured from as a 7-dimensional row vector.

\renewcommand{\thefootnote}{*} 
\begin{table}[h]
\centering
\begin{tabular}{l|llll}
\textbf{Dataset Name} & \textbf{Mean} & \textbf{Median} & \textbf{Mean Var.} & \textbf{Median Var.} \\\cline{1-5}
Raw         & 27.43    & 10.07    & 8329.96    & 7663.89 \\
Fourier       & 58.60\footnotemark    & -1.45    & 12229005.34    & 7418186.12 \\
Wavelet        & 1.30    & -.072    & 1887.5    & 1507.16 \\             
\end{tabular}
\caption{Dataset Summary Statistics}
\label{Tab:summary}
\end{table}
\footnotetext{There is an extremely small, but non-zero imaginary part, on the order of $10^{-19}i$}

\renewcommand{\thefootnote}{1}
The small but non-zero imaginary part in the Fourier data required implementation of methods from Trabelsi \textit{et al.}~\cite{trabelsi2017deep} to achieve acceptable results.

\section{Models}\label{model_descriptions}
In our experiments, we leveraged the following models:
\begin{itemize}
\item Fully Connected Neural Network
\item Convolutional Neural Network
\item Fourier Neural Network
\item Wavelet Neural Network
\item Random Forest
\item Support Vector Classifier	
\end{itemize}

The summary statistic dataset was not used with the convolutional neural network, nor was it used with the Fourier or wavelet neural networks because there is no spatial relationship between the data and so convolution offers no benefit.
Additionally, only the raw data was processed by the Fourier and Wavelet neural networks.
Though these networks are capable of processing the transformed data, there is no obvious benefit to transforming already-transformed data in-network.

All code\footnote{Code is available at the following url: \url{https://github.com/erickgalinkin/jhu_masters}} was written in Python, using the Tensorflow 2, PyTorch, and Scikit-learn libraries.
Only the baseline models - the random forest and support vector machine - described in \ref{other models} used the Scikit-learn library, and only the Wavelet Convolutional network described in \ref{wavelet cnn} used PyTorch.
The remaining models all used the Tensorflow framework.
In the case of our non-standard neural networks, we consider the work of Pratt~\cite{pratt2017fcnn} and Fujieda~\cite{fujieda2017wavelet}.
Both the Fourier and Wavelet neural networks take advantage of the convolution theorem - that is, given two functions $f$ and $g$,
\begin{align*}
(f * g)(t) & = \int_{-\infty}^{\infty} f(\tau)g(t-\tau)dt \\
& = \int_{\mathbb{R}^n}f(x) e^{-2\pi i x \nu} dx	 \cdot \int_{\mathbb{R}^n}g(x) e^{-2\pi i x \nu} dx\\	
& = \mathcal{F}\{f\}(\nu) \cdot \mathcal{F}\{g\}(\nu)
\end{align*}

and in the inverse, we get:
$$f \cdot g = \mathcal{F}^{-1}\{\mathcal{F}\{f\} * \mathcal{F}\{g\}\}$$

This allows us to avoid the high computational cost of performing a convolution via the sliding-tile method and instead potentially take advantage of the convolution theorem to perform convolution at the speed of the dot product.
We further elaborate on the architecture below.

For our neural networks, we use the Tensorflow standard stochastic gradient descent optimizer, with a learning rate of 0.001.
Results with other optimizers have been promising, and Adam~\cite{kingma2014adam} has been the optimizer of choice for many deep learning applications in the past few years, though we do not leverage it here. 
Hardware specifications on which these experiments ran is in Appendix~\ref{append:one}.

\subsection{Fully-Connected Neural Network}
The fully-connected neural network architecture is a basic multi-layer perceptron which accepts a 100-dimensional row vector.
This vector is then fed to three densely connected hidden layers, each with 256 ReLU-activated neurons.
The fourth and final output neuron is a single sigmoid-activated layer, which provides a probability of maliciousness.

\subsection{Standard Convolutional Neural Network}
Our convolutional neural network is a sequential model which accepts a 100-dimensional row vector as input.
This input is processed by two convolutional layers, both with 256 neurons.
The first has a kernel size of 5 and a stride size of 1, and the second has a kernel size of 3 with a stride of size 1.
The output of the second convolutional layer is processed by two densely connected layers of 128 neurons each. 
The final layer consists of a sigmoid-activated output layer, the same as the fully-connected neural network.

\subsection{Fourier Convolutional Neural Network}
Our Fourier ``Convolutional'' neural network is identical architecturally to our standard convolutional neural network, only with the convolutional layers replaced by Fourier layers.
Here, we put the word convolutional in quotes due to the fact that no actual convolution is performed.
To be more intellectually honest, we should refer to this network instead as a ``Fourier Transform Cross Product Network'', though this may confuse readers unfamiliar with the relationship.
In the interest of broad understanding, the term convolutional neural network is used when it helps clarify meaning even in spite of being a slight misnomer.

Specifically, the Fourier Convolutional Neural Network leverages a custom Fourier layer, which moves the data into Fourier space via the Fast Fourier Transform and then multiplies the transpose of the weight matrix with the input to the matrix.
Specifically, given an input $X^{(n)}$ where the superscript is not an exponent, but instead indicates the layer of the input, the Fourier layer $\ell$ acts on $X$ to give an input to our next layer:
\begin{equation}
X^{(n+1)} = \ell^{(n)}(X^{(n)}) = \sigma(\mathcal{F}^{-1}(\mathcal{F}(X^{(n)})\cdot \mathbf{W}^{(n)\top}))
\end{equation}
\noindent Where $\mathcal{F}$ is the Fast Fourier Transform, $\sigma$ is the activation function - ReLU in this case - and $\mathbf{W}$ is the weight matrix for layer n.

\subsection{Wavelet Convolutional Neural Network} \label{wavelet cnn}
The Wavelet Convolutional Neural Network implements similar functionality to our Fourier Neural Network, using the Discrete Wavelet Transform in lieu of the Fourier transform.
Due to the fact that there is a time component and a frequency component, the wavelet neural network has a different in-layer dimensionality than our other models but is otherwise identical.

In our Wavelet Convolutional Neural Network, we take a 100-dimensional row vector as input.
This input is then sent to the ``wavelet layer'' where it undergoes a Daubechies discrete wavelet transform.
There are a very large number of wavelets which can be used in the discrete wavelet transform~\cite{mallat1999wavelet} but the Daubechies wavelet is easy to put into practice and has a unique inverse everywhere, so we use it here.
The output is cast to a tensor which is multiplied against the transpose of the weight tensor.
This output then undergoes an inverse discrete wavelet transform with respect to the same mother wavelet.

\subsection{Baseline Models} \label{other models}
Two baseline models were considered on these datasets.
The first is the random forest model provided in the Scikit-learn library with no hyperparameter tuning.
Decision tree models are generally good at classification tasks~\cite{hastie01statisticallearning} but are weak classifiers which are sensitive to variance.
Random forests are the result of averaging a large collection of de-correlated trees and provide a good benchmark as a na\"ive model - in the respect that it is untuned - for classification.
Random forests are also performant in the respect that they train and evaluate examples quickly, relative to neural networks.
This makes them common for use in industry.

The other benchmark model is a Support Vector Classifier, again provided by the Scikit-learn library.
The rationale for using a Support Vector Machine is that we wanted to see if some hyperplane could be learned which would separate the data.
This model was again, na\"ive in the respect that it was merely the ``out of the box'' model, and so the classifier was built on top of the radial basis function kernel.
Details of the Support Vector Classifier can be found in James~\cite{james14introduction} or the Scikit-learn documentation~\cite{scikit-learn}.


\section{Results} \label{malware results}
We split the results here into three subsections for clarity, first presenting and discussing the malware data transformations with respect to the algorithms they were tested on. 
We then turn to the ways that all six architectures performed on the raw data.
Finally, we discuss the information plane of our neural networks.

\subsection{Malware Dataset Transformation}
Our results for the transformed datasets, contained in Table~\ref{Tab:malware_test} show our test accuracy and the mean time per batch for each neural network.
The time per batch is not available for the baseline models.

\begin{table}[h]
\centering	
\begin{tabular}{l|ll}
\textbf{Data and Architecture Combination} & \textbf{Test Accuracy} & \textbf{Mean Step Time} ($\mu$s) \\\cline{1-3}
Raw, Fully-Connected NN            & 63.40\%         & 13\\
Summary, Fully-Connected NN        & 55.14\%         & 13\\
Fourier, Fully-Connected NN        & 59.88\%         & 12\\
Wavelet, Fully-Connected NN        & 61.95\%         & 12\\
\hline
Raw, Convolutional NN              & 72.89\%         & 54\\
Fourier, Convolutional NN          & 70.81\%         & 56\\
Wavelet, Convolutional NN          & 70.77\%         & 52\\
\hline
Raw, Random Forest                 & 80.28\%         & N/A\\ 
Summary, Random Forest             & 76.91\%         & N/A\\
Fourier, Random Forest             & 79.63\%         & N/A\\
Wavelet, Random Forest             & 79.80\%         & N/A\\
\hline
Raw, Support Vector Classifier     & 65.77\%         & N/A\\    
Summary, Support Vector Classifier & 55.28\%         & N/A\\  
Fourier, Support Vector Classifier & 55.28\%         & N/A\\  
Wavelet, Support Vector Classifier & 55.28\%         & N/A           
\end{tabular}
\caption{Classifier accuracy on transformed datasets}
\label{Tab:malware_test}
\end{table}

In terms of accuracy, we find that the random forest on the raw data performs best, followed closely by the random forest on the wavelet-transformed data, and third the random forest trained on the Fourier-transformed data. 
On all datasets, the random forest classifier outperforms all other classifiers on that same dataset.
Notably, when we compare accuracy by model, we find that for the fully-connected neural network, our maximum average accuracy is 63.40\%, while our minimum average accuracy is given by the summary statistics.
Excluding the summary statistic data, the difference between the highest average accuracy and lowest average accuracy for fully-connected neural networks is 3.52\%, a very small margin. 
Comparatively, for the convolutional neural network, our delta is 2.12\%, again - quite small. 
Similarly, the random forest performs near the 80\% mark universally, irrespective of representation, and performs worst on the summary statistic dataset.

\subsection{In-network Data Transformation}
For our in-network data transformations, we consider only the raw dataset across all six architectures.
The test accuracy and mean time per batch for each neural network are contained in Table~\ref{Tab:test_arch}.

\begin{table}[h]
\begin{tabular}{l|ll}
\textbf{Architecture}  & \textbf{Test Accuracy} & \textbf{Mean Step Time} ($\mu$s) \\\cline{1-3}
Fully-Connected NN            & 63.40\%         & 13\\
Convolutional NN              & 72.89\%         & 54\\  
Fourier NN                    & 63.27\%         & 143\\
Wavelet NN                    & 74.85\%         & 228\\
Random Forest                 & 80.28\%         & N/A\\ 
Support Vector Classifier     & 65.77\%         & N/A       
\end{tabular}
\caption{All classifier accuracy on raw dataset only}
\label{Tab:test_arch}	
\centering
\end{table}

It is worth noting that one of the primary motivations for replacing the sliding-tile convolution method with a Fourier or Wavelet method is the performance gains identified by others~\cite{pratt2017fcnn}.
However, as we show, the Fourier and Wavelet networks are significantly slower than their untransformed counterparts on this dataset.
We conclude that the computational overhead of performing a transform and its corresponding inverse transform outweighs the speed-up gained by eliminating the sliding-tile convolution on smaller datasets, and the method as demonstrated in Pratt~\cite{pratt2017fcnn} should be reserved for relatively large images, where convolution is already slow.
In our case, we see a 2.65x increase in step time between a standard convolution and the Fourier method. 
Unfortunately, our activation functions do not behave nicely in the Fourier or Wavelet domain, as these functions operate linearly with respect to the space and so an inverse transform must be applied. 
The question of using a novel convolution operator and conducting the activation in that space has been addressed by Chakraborty~\cite{chakraborty2019surreal} but goes well beyond the question of simply adapting an activation function to the Fourier or Wavelet space.
The search for a spectral activation function remains an open question.

\subsection{Malware Dataset Information Plane Analysis}\label{infoplane}
Figure~\ref{fig:infoplane example} displays a zoomed-in view of the information plane for our malware dataset and neural network.
On the x axis is the mutual information $I(X;M)$, computed as described in Section~\ref{MI computation}.
On the y axis is the mutual information $I(Y;M)$. 
Optimally, we want to see high values on the y axis and lower values on the x axis for each layer - this would suggest that the learned representation in the neural network, $M$, requires relatively little data about $X$ to reliably predict $Y$.
In this figure, each layer is plotted independently. 

\begin{figure}[h!]
\begin{center}
\includegraphics[width=\textwidth]{fc_raw_large}
\caption{Large plot for Fully-Connected Neural Network Information Plane on Raw Data. Produced using the upper bound and binning methodology from Saxe~\cite{saxe2019information} over 1000 epochs.}
\label{fig:infoplane example}
\centering
\end{center}
\end{figure}

The cluster of data points on the lower-left hand side represent the output layer, which gains slightly better predictive ability about the data throughout the 1000 epochs. 
The shift toward the right in later epochs is suggestive of overfitting the dataset, and $M$ containing more information about $X$.
Meanwhile, the shift upward, particularly early on, indicates the network improving the amount of mutual information between $M$ and $Y$.
We define the information plane as in Tishby~\cite{tishby2015deep}: the plane of the mutual information values that each layer preserves on the input and output.
In the upper right of the plot, we see what appears to be a single point - this is all three hidden layers of the neural network, which do not see any change in mutual information.
The low level of mutual information may be due to the weak correlation relationship between $X$ and $Y$, which has a bivariate correlation of 0.2629 for the raw data.

We can see in Figure~\ref{fig:malware infoplane fc}, subplots A, B, and C, that the amount of information changes very little. There is a high level of mutual information about $Y$ and $X$ captured in the hidden layers, while the output layer has almost no information about $Y$ and only learns less than 2 bits of information about $X$.
Given how similar the accuracies for the fully-connected neural network were - as can be seen in Table~\ref{Tab:malware_test} and how similar subplots A, B, and C are in Figure~\ref{fig:malware infoplane fc}, it's clear that the learned representations capture the same amount of information about the target labels.
With respect to our summary dataset information plane in subplot D, we note that the graph looks more like a scatter plot than a line chart, seemingly because of the nature of the transformation - that is, the entire representation of the data is changed. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=\textwidth]{malware_infoplane_fc_new}
\caption{Fully-Connected Neural Network Information Plane for four malware data sets}
\label{fig:malware infoplane fc}
\centering
\end{center}
\end{figure}

As we note in our discussion of minimum necessary information in Equation~\ref{eqn:MNI}, we achieve optimality only when $X$ uniquely determines $Y$, which does not appear to be the case for our dataset. 
It is worth noting that all of the information planes in Figure~\ref{fig:malware infoplane fc} aside from the summary data in subplot D do not change their mutual information for the hidden layers and converge to the same mutual information for the output layer - the only layer which changes.
It was experimentally verified that the weights and entropies of each individual layer did change throughout training, but the mutual information for the hidden layers remained stationary across 1000 epochs.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=\textwidth]{malware_infoplane_conv}
\caption{Information Plane for Convolutional neural network for three data sets}
\label{fig:malware infoplane conv}
\centering
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
\includegraphics[width=\textwidth]{malware_infoplane_fourier_wavelet}
\caption{Information Plane for Fourier and Wavelet neural networks for raw data}
\label{fig:malware infoplane fourier-wavelet}
\centering
\end{center}
\end{figure}


The similarity of the plots in Figure~\ref{fig:malware infoplane fc}, Figure~\ref{fig:malware infoplane conv}, and Figure~\ref{fig:malware infoplane fourier-wavelet} is not a coincidence, and the captured mutual information about the labels in the output layer is within a fraction of a bit for all of our networks. 
The only deviation can be observed in Figure~\ref{fig:malware infoplane conv} subplot C, the information plane for the convolutional neural network trained on the wavelet data. 
This effect happens in the convolutional layers, and only on the wavelet-transformed data.
The cause of this change is unknown, and was not investigated.
Despite this difference, the dense hidden layers and the output layer converge to the same points in the information plane as the other networks and datasets.

In Figure~\ref{fig:malware infoplane fourier-wavelet}, we observe that the convolution theorem, which preserved our accuracy quite well, also seems to preserve mutual information.
Both subplots A and B mimic the information plane of the fully-connected neural networks and have nearly the same information plane graph as the convolutional neural network as seen in Figure~\ref{fig:malware infoplane conv}, Subplots A and B.

Some of the difference between the initial mutual information states for network and dataset combinations can be explained by the stochasticity in neural networks - that is, the weights of the network are initialized randomly, and samples are chosen at random. 
As a result, it is crucial to look at how the networks converge, and after 1000 epochs, the key factor between networks appears to be architecture - that is, whether the network is feedforward or convolutional - rather than the representation of the data itself.
We conclude that the weights learned by the neural network are thus related far more strongly to latent representations in the data than they are to the specific values of the input data.
Thus, if we are to improve our malware detection using neural networks, we will likely need to find a superior architecture or try to capture a different artifact of the relationship between malware and benign software.

Further, for all of our neural networks, our information plane is quite similar, and converges to exactly the same value for mutual information in the densely connected hidden layers.
The exception is the summary dataset, which is the only dataset whose representation is not the result of a homeomorphic transformation.
We also note that this is the dataset with the worst evaluation accuracy.