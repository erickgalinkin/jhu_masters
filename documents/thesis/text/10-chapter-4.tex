\chapter{Information Theory and Neural Networks}
\label{chap:four}
%\newpage

\section{A Whirlwind Tour of Information Theory}
Given our very consistent accuracy in the face of transformations, we want to answer the question of why.
As we discuss in \ref{chap:three}, the summary statistics of these datasets change substantially, but it is difficult to explain the consistency of the accuracy in light of those results.
As a way to begin trying to explain those results, we turn to information theory and the concepts of entropy and mutual information.
For the following proofs, all random variables are assumed to be continuous. 
This is because it more naturally and more obviously extends to the discrete case than the other way around.

First, we define entropy as the measure of uncertainty of a random variable.
Let $X$ be a random variable with cumulative distribution function $F_X(x) = Pr\{X = x\}$, and the probability density function for X, $f(x) = F'(x)$ where the derivative of the CDF exists and $\int_{-\infty}^{\infty}f(x) = 1$.
We let $S$ be the support set for $f(x)$, that is, the set of values for which $f(x) > 0$.
Then the differential entropy of $X$, represented $H(X)$ is defined as
$$ h(X) = -\int_{S} f(x) \log_2 f(x) dx$$
Here our logarithm is to the base 2, as information is most commonly represented as bits. 

In the case where we are examining two random variables, for example, a dataset and its labels, we may want to consider the joint and conditional entropy of those random variables.
Given a joint distribution $f_{X,Y}(x, y)$ of two random variables $X, Y$, we can define the joint differential entropy $h(X, Y)$ as:
$$ h(X, Y) = - \int \int f_{X,Y}(x, y) \log_2 f_{X,Y}(x, y) dx dy$$

and the conditional differential entropy $h(Y | X)$ as:
$$ h(Y|X) = -\int \int f_{X,Y}(x, y) \log_2 f_{(Y|X)}(y|x) dx dy$$

For the continuous case, we define the Kullback-Leibler divergence $D(f||g)$ between two densities $f$ and $g$ to be:
$$D(f||g) = \int f \log_2 \frac{f}{g}$$
which is finite if and only if the support set of $f$ is in the support set of $g$.

Armed with the above knowledge, we define mutual information, $I(X; Y)$ as the relative entropy between the joint distribution and the product distribution.
$$I(X; Y) = \int \int f(x, y) \log_2 \frac{f_{X,Y}(x, y)}{f_{X}(x) f_{Y}(y)} dx dy = D(f_{X,Y}(x, y)||f_{X}(x)f_{Y}(y))$$

Detailed derivation of the above results are all available in \cite{coverthomas2006}.

\section{Invariance of Mutual Information in Neural Networks}

We begin by proving the invariance of mutual information under homeomorphism.

\begin{theorem}[Invariance of Mutual Information under Homeomorphism]
Take two random variables $X$ and $Y$ where $Y$ is the set of labels of $X$.
Let $X' = \psi(X)$, where $\psi$ is a smooth and uniquely invertible map.
Then $I(X'; Y) = I(X; Y)$.
\end{theorem}

\begin{proof}
Given the Jacobi determinant $J_X = ||\partial X/ \partial X'||$, we observe that the joint distribution of $X'$ and $Y$ is given by: $f_{X', Y}(x', y) = J_X(x')f(x, y)$

\begin{align*}
I(X'; Y) & = \int \int f(x', y) \log_2 \frac{f(x', y)}{f_{X'}(x') f_{Y}(y)} dy dx'\\
& = \int \int J_X(x')f(x, y) \log_2 \frac{J_X(x')f(x, y)}{J_X(x')f_{X}(x)f_{Y}(y)} dy dx'\\
& = \int \int f(x, y) \log_2 \frac{f(x, y)}{f_{X}(x) f_{Y}(y)} dy dx \\
& = I(X; Y)
\end{align*}
\end{proof}
concluding our proof that mutual information is transformation invariant. 



Given our results detailed in chapter \ref{chap:three}, we can see that 